{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install haystack-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so i can use .env-file where i stored my API-key\n",
    "# pip install python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai[mistral] in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (2.9.0)\n",
      "\u001b[33mWARNING: haystack-ai 2.9.0 does not provide the extra 'mistral'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: haystack-experimental in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (0.5.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (3.1.4)\n",
      "Requirement already satisfied: lazy-imports in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (0.4.0)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (10.6.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (3.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.56.1 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (1.60.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (2.2.2)\n",
      "Requirement already satisfied: posthog in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (3.11.0)\n",
      "Requirement already satisfied: pydantic in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (9.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from haystack-ai[mistral]) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from openai>=1.56.1->haystack-ai[mistral]) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from openai>=1.56.1->haystack-ai[mistral]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from openai>=1.56.1->haystack-ai[mistral]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from openai>=1.56.1->haystack-ai[mistral]) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from openai>=1.56.1->haystack-ai[mistral]) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from pydantic->haystack-ai[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from pydantic->haystack-ai[mistral]) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from jinja2->haystack-ai[mistral]) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from pandas->haystack-ai[mistral]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from pandas->haystack-ai[mistral]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from python-dateutil->haystack-ai[mistral]) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from posthog->haystack-ai[mistral]) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from posthog->haystack-ai[mistral]) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from requests->haystack-ai[mistral]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from requests->haystack-ai[mistral]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from requests->haystack-ai[mistral]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from requests->haystack-ai[mistral]) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai[mistral]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai[mistral]) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install \"haystack-ai[mistral]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement haystack-ai-mistral (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for haystack-ai-mistral\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install haystack-ai-mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'haystack_mistral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack_mistral\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralChatGenerator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'haystack_mistral'"
     ]
    }
   ],
   "source": [
    "from haystack_mistral.components.generators import MistralChatGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haystack-integrations-mistral\n",
      "  Cloning https://github.com/deepset-ai/haystack-core-integrations.git to /private/var/folders/9l/ng7prn7d7z1gpmzs8nj4klgm0000gn/T/pip-install-_eiwj8nj/haystack-integrations-mistral_fdb64ddd2eb74f37860a8817aaa892f1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack-core-integrations.git /private/var/folders/9l/ng7prn7d7z1gpmzs8nj4klgm0000gn/T/pip-install-_eiwj8nj/haystack-integrations-mistral_fdb64ddd2eb74f37860a8817aaa892f1\n",
      "  Resolved https://github.com/deepset-ai/haystack-core-integrations.git to commit 4b003f2a4cf75d85fca6436fb91a4fe6d46e6006\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package haystack-integrations-mistral produced metadata for project name mistral-haystack. Fix your #egg=haystack-integrations-mistral fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mgit+https://github.com/deepset-ai/haystack-core-integrations.git#egg=haystack-integrations-mistral&subdirectory=integrations/mistral\u001b[0m: \u001b[33mRequested mistral-haystack from git+https://github.com/deepset-ai/haystack-core-integrations.git#egg=haystack-integrations-mistral&subdirectory=integrations/mistral has inconsistent name: expected 'haystack-integrations-mistral', but metadata has 'mistral-haystack'\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement haystack-integrations-mistral (unavailable) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for haystack-integrations-mistral (unavailable)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"git+https://github.com/deepset-ai/haystack-core-integrations.git#egg=haystack-integrations-mistral&subdirectory=integrations/mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads the varialbe from .env-file\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to see if LLM is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the \"best\" French meal can be subjective, as it depends on personal preferences. However, one of the most iconic and beloved French dishes is \"Coq au Vin.\" Here is a short JSON object with the name and main ingredients:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Coq au Vin\",\n",
      "  \"ingredients\": [\n",
      "    \"Chicken\",\n",
      "    \"Red wine\",\n",
      "    \"Bacon\",\n",
      "    \"Onions\",\n",
      "    \"Carrots\",\n",
      "    \"Garlic\",\n",
      "    \"Mushrooms\",\n",
      "    \"Thyme\",\n",
      "    \"Bay leaves\",\n",
      "    \"Flour\",\n",
      "    \"Butter\",\n",
      "    \"Salt\",\n",
      "    \"Pepper\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This dish is a classic French stew where chicken is slow-cooked in red wine with an assortment of vegetables and herbs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "#chat_response = client.chat.complete(\n",
    "#    model= model, messages = [{\n",
    "#            \"role\": \"user\",\n",
    "#            \"content\": \"What is the best French meal? Return the name and the ingredients in short JSON object.\",\n",
    "#        },])\n",
    "#print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'haystack_integrations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentWriter\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#from haystack_integrations.components.embedders.mistral import MistralDocumentEmbedder, MistralTextEmbedder\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack_integrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralChatGenerator\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Write documents to InMemoryDocumentStore\u001b[39;00m\n\u001b[1;32m     19\u001b[0m document_store \u001b[38;5;241m=\u001b[39m InMemoryDocumentStore()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'haystack_integrations'"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline, Document\n",
    "from haystack.utils import Secret\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "\n",
    "#from haystack.components.builders import DynamicChatPromptBuilder\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "#from haystack_integrations.components.embedders.mistral import MistralDocumentEmbedder, MistralTextEmbedder\n",
    "from haystack_integrations.components.generators.mistral import MistralChatGenerator\n",
    "\n",
    "\n",
    "\n",
    "# Write documents to InMemoryDocumentStore\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents([\n",
    "    Document(content=\"My name is Jean and I live in Paris.\"), \n",
    "    Document(content=\"My name is Mark and I live in Berlin.\"), \n",
    "    Document(content=\"My name is Giorgio and I live in Rome.\")\n",
    "])\n",
    "\n",
    "# 3. Write a Prompt Template\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 4. Create the components\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "llm = MistralChatGenerator(api_key=Secret.from_token(api_key), \n",
    "                           model='mistral-large-latest')\n",
    "\n",
    "# 5. Create a pipeline instance\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "# 6. Add components to the pipeline\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "# 7. Connect individual components\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "# 8. Ask a question and define the results\n",
    "question = \"Who lives in Paris?\"\n",
    "results = rag_pipeline.run(\n",
    "    {\n",
    "        \"retriever\": {\"query\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results[\"llm\"][\"replies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MistralChatGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m prompt_builder \u001b[38;5;241m=\u001b[39m PromptBuilder(template\u001b[38;5;241m=\u001b[39mprompt_template)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#llm = OpenAIGenerator(api_key=Secret.from_token(api_key))\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mMistralChatGenerator\u001b[49m(api_key\u001b[38;5;241m=\u001b[39mSecret\u001b[38;5;241m.\u001b[39mfrom_token(api_key), \n\u001b[1;32m     31\u001b[0m                            model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral-large-latest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m rag_pipeline \u001b[38;5;241m=\u001b[39m Pipeline()\n\u001b[1;32m     34\u001b[0m rag_pipeline\u001b[38;5;241m.\u001b[39madd_component(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MistralChatGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline, Document\n",
    "from haystack.utils import Secret\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "\n",
    "# Write documents to InMemoryDocumentStore\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents([\n",
    "    Document(content=\"My name is Jean and I live in Paris.\"), \n",
    "    Document(content=\"My name is Mark and I live in Berlin.\"), \n",
    "    Document(content=\"My name is Giorgio and I live in Rome.\")\n",
    "])\n",
    "\n",
    "# Build a RAG pipeline\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "#llm = OpenAIGenerator(api_key=Secret.from_token(api_key))\n",
    "llm = MistralChatGenerator(api_key=Secret.from_token(api_key), \n",
    "                           model='mistral-large-latest')\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "# Ask a question\n",
    "question = \"Who lives in Paris?\"\n",
    "results = rag_pipeline.run(\n",
    "    {\n",
    "        \"retriever\": {\"query\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results[\"llm\"][\"replies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DynamicChatPromptBuilder' from 'haystack.components.builders' (/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/haystack/components/builders/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMessage\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Secret\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicChatPromptBuilder\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextFileToDocument\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentSplitter\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DynamicChatPromptBuilder' from 'haystack.components.builders' (/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/haystack/components/builders/__init__.py)"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.utils.auth import Secret\n",
    "\n",
    "from haystack.components.builders import DynamicChatPromptBuilder\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack_integrations.components.embedders.mistral import MistralDocumentEmbedder, MistralTextEmbedder\n",
    "from haystack_integrations.components.generators.mistral import MistralChatGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "docs = TextFileToDocument().run(sources=[\"essay.txt\"])\n",
    "split_docs = DocumentSplitter(split_by=\"passage\", split_length=2).run(documents=docs[\"documents\"])\n",
    "embeddings = MistralDocumentEmbedder(api_key=Secret.from_token(api_key)).run(documents=split_docs[\"documents\"])\n",
    "DocumentWriter(document_store=document_store).run(documents=embeddings[\"documents\"])\n",
    "\n",
    "\n",
    "text_embedder = MistralTextEmbedder(api_key=Secret.from_token(api_key))\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = DynamicChatPromptBuilder(runtime_variables=[\"documents\"])\n",
    "llm = MistralChatGenerator(api_key=Secret.from_token(api_key), \n",
    "                           model='mistral-large-latest')\n",
    "\n",
    "chat_template = \"\"\"Answer the following question based on the contents of the documents.\\n\n",
    "                Question: {{query}}\\n\n",
    "                Documents: \n",
    "                {% for document in documents %}\n",
    "                    {{document.content}}\n",
    "                {% endfor%}\n",
    "                \"\"\"\n",
    "messages = [ChatMessage.from_user(chat_template)]\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "\n",
    "question = \"What were the two main things the author worked on before college?\"\n",
    "\n",
    "result = rag_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"template_variables\": {\"query\": question}, \"prompt_source\": messages},\n",
    "        \"llm\": {\"generation_kwargs\": {\"max_tokens\": 225}},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"llm\"][\"replies\"][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import requests\n",
    "import numpy as np\n",
    "#import faiss\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "#api_key= getpass(\"Type your API Key\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "f = open('essay.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m     embeddings_batch_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      7\u001b[0m           model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m           inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\n\u001b[1;32m      9\u001b[0m       )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_batch_response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m---> 11\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mget_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     embeddings_batch_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      7\u001b[0m           model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m           inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\n\u001b[1;32m      9\u001b[0m       )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_batch_response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m---> 11\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mget_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks])\n",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m, in \u001b[0;36mget_text_embedding\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_embedding\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     embeddings_batch_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral-embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_batch_response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/mistralai/embeddings.py:99\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, inputs, model, encoding_format, retries, server_url, timeout_ms, http_headers)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res_text, http_res\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    103\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}"
     ]
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "print(len(chunks))\n",
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "          model=\"mistral-embed\",\n",
    "          inputs=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a Pipeline\n",
    "## 1.1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document, Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "text_embedder = SentenceTransformersTextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is an example of how you'd add the components initialized in step 2 above:\n",
    "query_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "query_pipeline.add_component(\"retriever\", retriever)\n",
    "\n",
    "rag_pipeline.add_component(\"llm\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Connect components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Run the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elisabeth_python-umgebung_nr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

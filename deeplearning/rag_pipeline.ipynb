{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads the varialbe from .env-file\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to see if LLM is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Curry-Kaviar-Strudel mit Udo-Nudeln und Parmesan\",\n",
      "  \"ingredients\": [\n",
      "    \"Kaviar\",\n",
      "    \"Udo-Nudeln\",\n",
      "    \"Parmesan\",\n",
      "    \"Strudelteig\",\n",
      "    \"Currypulver\"\n",
      "  ],\n",
      "  \"preparation\": [\n",
      "    \"Den Strudelteig ausrollen und mit Currypulver bestreuen.\",\n",
      "    \"Kaviar gleichmäßig auf dem Strudelteig verteilen.\",\n",
      "    \"Den Strudelteig aufrollen und in eine gefettete Backform legen.\",\n",
      "    \"Im vorgeheizten Ofen bei 180°C für etwa 20-25 Minuten backen, bis er goldbraun ist.\",\n",
      "    \"Während der Strudel im Ofen ist, die Udo-Nudeln nach Packungsanweisung kochen.\",\n",
      "    \"Die gekochten Udo-Nudeln abgießen und mit Parmesan bestreuen.\",\n",
      "    \"Den fertigen Strudel aus dem Ofen nehmen und in Scheiben schneiden.\",\n",
      "    \"Die Strudelscheiben auf den Udo-Nudeln anrichten und sofort servieren.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Erstelle ein Rezept. Die Zutaten für das Rezept sind Kaviar, Udo-Nudeln, Parmesan, Strudelteig, Currypulver. Return the name, the ingredients and die Zubereitung in short JSON object.\",\n",
    "    }]\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model= model, \n",
    "    messages = messages,\n",
    "    response_format = {\"type\": \"json_object\",}\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for RAG that is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n",
      "Rate limit erreicht, warte 1 Sekunden...\n"
     ]
    }
   ],
   "source": [
    "# https://docs.mistral.ai/guides/rag/\n",
    "\n",
    "from mistralai import Mistral\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "#api_key= getpass(\"Type your API Key\")\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "f = open('essay.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "print(\"chunk number is\", len(chunks))\n",
    "\n",
    "# def get_text_embedding(input):\n",
    "#     try:\n",
    "#         embeddings_batch_response = client.embeddings.create(\n",
    "#             model=\"mistral-embed\",\n",
    "#             inputs=input\n",
    "#         )\n",
    "#         return embeddings_batch_response.data[0].embedding\n",
    "#     #claude.ai did the following code\n",
    "#     except Exception as e:  \n",
    "#         print(f\"Fehler beim Embedding: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Prüfen auf None-Werte und filtern\n",
    "# text_embeddings = [get_text_embedding(chunk) for chunk in chunks]\n",
    "# text_embeddings = [emb for emb in text_embeddings if emb is not None]\n",
    "# text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "# # Optional: Dimensionen prüfen\n",
    "# print(f\"Embedding-Dimensionen: {text_embeddings.shape}\")\n",
    "# # Das Problem ist ein Rate Limit der Mistral API. Hier ist die Lösung mit Wartezeit zwischen den Anfragen:\n",
    "\n",
    "import time\n",
    "\n",
    "def get_text_embedding(input, retry_delay=1):\n",
    "    while True:\n",
    "        try:\n",
    "            embeddings_batch_response = client.embeddings.create(\n",
    "                model=\"mistral-embed\",\n",
    "                inputs=input\n",
    "            )\n",
    "            return embeddings_batch_response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            if \"rate limit exceeded\" in str(e).lower():\n",
    "                print(f\"Rate limit erreicht, warte {retry_delay} Sekunden...\")\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            print(f\"Fehler beim Embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "text_embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = get_text_embedding(chunk)\n",
    "    if embedding is not None:\n",
    "        text_embeddings.append(embedding)\n",
    "    time.sleep(0.5)  # Zusätzliche Pause zwischen Anfragen\n",
    "\n",
    "text_embeddings = np.array(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two main things the author worked on before college were writing (specifically short stories) and programming.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"What were the two main things the author worked on before college?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "\n",
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other Example for RAG that is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 3it [00:03,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two main things the author worked on before college were writing and programming. He wrote short stories, although he admits they were awful, and he also wrote essays about various topics. On the programming side, he worked on spam filters and began to develop his interest in artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "# Example is from https://docs.mistral.ai/guides/rag/#rag-with-haystack\n",
    "\n",
    "# changes were made with help of claude.ai\n",
    "#  1. DynamicChatPromptBuilder replaced by ChatPromptBuilder\n",
    "#  1.a. ChatPromptBuilder doesn't use runtime_variables or prompt_source parameters\n",
    "#  1.b. Instead, it takes the messages directly as template\n",
    "#  1.c. The template variables are passed directly to the run method\n",
    "#  2.a. Changed the import from DynamicChatPromptBuilder to ChatPromptBuilder\n",
    "#  2.b. Removed the runtime_variables parameter from the prompt builder initialization\n",
    "#  2.c. Changed prompt_source to template in the pipeline run parameters\n",
    "#  2.d. Added documents as a template variable in the run parameters\n",
    "#  3. Ah, I see the issue. The ChatPromptBuilder doesn't have a documents input socket like DynamicChatPromptBuilder did. Instead, we need to pass the documents through the template_variables. Here's the corrected code:\n",
    "#  3.a. Removed the connection between retriever and prompt_builder since ChatPromptBuilder doesn't have a documents input\n",
    "#  3.b. Added the documents directly in the template_variables\n",
    "#  3.c. Added retriever query parameter in the pipeline run\n",
    "#  4. Ah, I see the issue. For the InMemoryEmbeddingRetriever, we need to provide the query_embedding, not the raw query. Let me correct the code:\n",
    "#  4.a. Removed the direct \"retriever\" input from the pipeline run\n",
    "#  4.b. In the template_variables, we now get the documents by:\n",
    "#  4.b.a. First getting the embedding for the question using text_embedder\n",
    "#  4.b.b. Then using that embedding to retrieve documents from the retriever\n",
    "#  4.b.c. Passing those retrieved documents to the template_variables\n",
    "#  5. Ah, I see. There's been an API change in Haystack where we need to use .text instead of .content to access the message content. Here's the corrected code:\n",
    "\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.utils.auth import Secret\n",
    "\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack_integrations.components.embedders.mistral import MistralDocumentEmbedder, MistralTextEmbedder\n",
    "from haystack_integrations.components.generators.mistral import MistralChatGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "docs = TextFileToDocument().run(sources=[\"essay.txt\"])\n",
    "split_docs = DocumentSplitter(split_by=\"passage\", split_length=2).run(documents=docs[\"documents\"])\n",
    "embeddings = MistralDocumentEmbedder(api_key=Secret.from_token(api_key)).run(documents=split_docs[\"documents\"])\n",
    "DocumentWriter(document_store=document_store).run(documents=embeddings[\"documents\"])\n",
    "\n",
    "text_embedder = MistralTextEmbedder(api_key=Secret.from_token(api_key))\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = ChatPromptBuilder()\n",
    "llm = MistralChatGenerator(api_key=Secret.from_token(api_key), \n",
    "                          model='mistral-small')\n",
    "\n",
    "chat_template = \"\"\"Answer the following question based on the contents of the documents.\\n\n",
    "                Question: {{query}}\\n\n",
    "                Documents: \n",
    "                {% for document in documents %}\n",
    "                    {{document.content}}\n",
    "                {% endfor%}\n",
    "                \"\"\"\n",
    "messages = [ChatMessage.from_user(chat_template)]\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "\n",
    "question = \"What were the two main things the author worked on before college?\"\n",
    "result = rag_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\n",
    "            \"template\": messages,\n",
    "            \"template_variables\": {\n",
    "                \"query\": question,\n",
    "                \"documents\": retriever.run(query_embedding=text_embedder.run(text=question)[\"embedding\"])[\"documents\"]\n",
    "            }\n",
    "        },\n",
    "        \"llm\": {\"generation_kwargs\": {\"max_tokens\": 225}},\n",
    "    }\n",
    ")\n",
    "print(result[\"llm\"][\"replies\"][0].text)  # Changed from .content to .text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
